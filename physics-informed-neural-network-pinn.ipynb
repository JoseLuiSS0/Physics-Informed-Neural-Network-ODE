{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Physics-Informed Neural Network (PINN) para aproximar una Ecuación Diferencial.**\n\n*José Luis Sánchez Soto A01802409*\n\n**¿Qué es una PINN?**\n\nUna Physics-Informed Neural Network (PINN) es una red neuronal que aprende a resolver ecuaciones diferenciales incorporando la física dentro de la función de pérdida.\n\n**¿En qué se diferencia una PINN de una red neuronal tradicional?**\n\nUna red neuronal tradicional aprende a partir de datos etiquetados. Es decir, necesita pares de entrada–salida (x,y) para ajustar sus parámetros y minimizar un error entre sus predicciones y los datos reales.\n\nEn cambio, una Physics-Informed Neural Network (PINN) no depende únicamente de datos. Su entrenamiento incorpora directamente las leyes físicas del problema, representadas por la ecuación diferencial.\nEsto significa que la función de pérdida incluye el residuo de la ecuación diferencial, asegurando que la solución aprendida cumpla con la física del sistema.\n\n**Ecuación diferencial a resolver:**\n\n$y' = e^{x+y}, \\quad y(0) = -1$\n\n**Resolviendo analíticamente:**\n\n$y = -\\ln\\left(e + 1 - e^{x}\\right)$\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# Importamos las líbrerias necesarias\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T03:14:40.667446Z","iopub.execute_input":"2025-11-19T03:14:40.668136Z","iopub.status.idle":"2025-11-19T03:14:40.673143Z","shell.execute_reply.started":"2025-11-19T03:14:40.668103Z","shell.execute_reply":"2025-11-19T03:14:40.672044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Para empezar primero importamos las librerías que vamos a utilizar:\n- **TensorFlow**: Para crear y entrenar la red neuronal\n- **NumPy**: Para cálculos numéricos\n- **Matplotlib**: Para visualizar resultados","metadata":{}},{"cell_type":"code","source":"# Definimos la solución exacta para compararla después\ndef exact_solution(x):\n    return -np.log(np.e + 1 - np.exp(x))\n\n# Probamos la solución exacta en algunos puntos\nx_test = np.array([0.0, 0.5, 1.0])\ny_exact = exact_solution(x_test)\nprint(\"Solución exacta en algunos puntos:\")\nfor i in range(len(x_test)):\n    print(f\"x = {x_test[i]:.1f}, y = {y_exact[i]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T03:14:40.675807Z","iopub.execute_input":"2025-11-19T03:14:40.676395Z","iopub.status.idle":"2025-11-19T03:14:40.707244Z","shell.execute_reply.started":"2025-11-19T03:14:40.676365Z","shell.execute_reply":"2025-11-19T03:14:40.706065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Solución Exacta**\n\nDefinimos la solución analítica conocida para poder comparar con los resultados de nuestra red neuronal. Esto nos permite validar que la PINN está aprendiendo correctamente.","metadata":{}},{"cell_type":"code","source":"# Creamos la red neuronal\ndef create_simple_nn():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(20, activation='tanh', input_shape=(1,)),\n        tf.keras.layers.Dense(20, activation='tanh'),\n        tf.keras.layers.Dense(20, activation='tanh'),\n        tf.keras.layers.Dense(1)])\n    return model\n\nmodel = create_simple_nn()\nprint(\"Red neuronal creada:\")\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T03:14:40.708230Z","iopub.execute_input":"2025-11-19T03:14:40.708523Z","iopub.status.idle":"2025-11-19T03:14:40.930285Z","shell.execute_reply.started":"2025-11-19T03:14:40.708501Z","shell.execute_reply":"2025-11-19T03:14:40.929304Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Arquitectura de la Red Neuronal:**\n\nCreamos una red neuronal fully-connected (densa) con:\n- **1 neurona de entrada** (el valor de x)\n- **3 capas ocultas** con 20 neuronas cada una\n- **1 neurona de salida** (la predicción de y(x))\n\nUsamos **tanh** como la función de activación porque es suave y diferenciable, lo cual es importante para calcular derivadas.","metadata":{}},{"cell_type":"code","source":"# Función de pérdida que incorpora la física\ndef physics_loss(x, model):\n\n    #Calcula la pérdida física: cuánto se viola la ecuación diferencial\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y_pred = model(x)\n    \n    # Calcular derivada dy/dx usando Automatic Differentiation\n    dy_dx = tape.gradient(y_pred, x)\n    \n    # Ecuación diferencial: y' = e^(x+y)\n    # Queremos que: dy_dx - e^(x+y) = 0\n    diff_eq_loss = dy_dx - tf.exp(x + y_pred)\n    \n    # Pérdida como error cuadrático medio\n    return tf.reduce_mean(tf.square(diff_eq_loss))\n\ndef initial_condition_loss(model):\n\n    #Pérdida por condición inicial: y(0) debería ser -1\n    x0 = tf.constant([[0.0]])  # x = 0\n    y0_pred = model(x0)\n    y0_true = tf.constant([[-1.0]])  # y(0) = -1\n    return tf.reduce_mean(tf.square(y0_pred - y0_true))\n\ndef total_loss(x, model):\n    \n    #Pérdida total = Pérdida física + Pérdida condición inicial\n    loss_physics = physics_loss(x, model)\n    loss_initial = initial_condition_loss(model)\n    return loss_physics + loss_initial\n\nprint(\"Funciones de pérdida definidas\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T03:14:40.931169Z","iopub.execute_input":"2025-11-19T03:14:40.931492Z","iopub.status.idle":"2025-11-19T03:14:40.939168Z","shell.execute_reply.started":"2025-11-19T03:14:40.931468Z","shell.execute_reply":"2025-11-19T03:14:40.938192Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**La Pérdida Física**\n\nEn una red neuronal normal, solo tenemos datos de entrada y salida. En una PINN, incorporamos la ecuación diferencial directamente en la función de pérdida:\n\n1. **Pérdida Física**: Medimos cuánto viola nuestra solución la ecuación diferencial\n   $\\text{loss}_\\text{physics} = \\left(\\frac{dy}{dx} - e^{(x+y)}\\right)^2$\n\n2. **Pérdida Condición Inicial**: Aseguramos que $y(0) = -1$\n\n3. **Pérdida Total**: Suma de ambas pérdidas\n\nUsamos `GradientTape` de TensorFlow para calcular las derivadas automáticamente.","metadata":{}},{"cell_type":"code","source":"# Entrenar la PINN\ndef train_pinn(model, epochs=3000, learning_rate=0.001):\n    \n    # Entrena la PINN usando el optimizador Adam\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    \n    # Puntos de entrenamiento en el dominio [0, 1]\n    x_train = tf.constant(np.random.rand(1000, 1), dtype=tf.float32)\n    \n    # Historial de pérdidas\n    losses = []\n    \n    print(\"Comenzando el entrenamiento...\")\n    for epoch in range(epochs):\n        with tf.GradientTape() as tape:\n            loss = total_loss(x_train, model)\n        \n        # Calcular los gradientes y actualizar pesos\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        \n        losses.append(loss.numpy())\n        \n        if epoch % 250 == 0:\n            print(f\"Época {epoch}: Pérdida = {loss.numpy():.6f}\")\n    \n    return losses\n\n# Entrenar el modelo\nlosses = train_pinn(model)\nprint(\"Entrenamiento completado\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T03:14:40.940156Z","iopub.execute_input":"2025-11-19T03:14:40.940533Z","iopub.status.idle":"2025-11-19T03:17:43.629516Z","shell.execute_reply.started":"2025-11-19T03:14:40.940500Z","shell.execute_reply":"2025-11-19T03:17:43.628327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Proceso de Entrenamiento**\n\nEntrenamos la red neuronal minimizando la pérdida total:\n- **Optimizador**: Adam (eficiente para redes neuronales)\n- **Tasa de aprendizaje**: 0.001\n- **Épocas**: 3000 iteraciones\n- **Puntos de entrenamiento**: 1000 puntos aleatorios en [0, 1]\n\nEn cada época:\n1. Calculamos la pérdida total\n2. Calculamos gradientes con `GradientTape`\n3. Actualizamos los pesos de la red","metadata":{}},{"cell_type":"code","source":"# Evaluar y comparar con solución exacta\n\n# Generar puntos de prueba\nx_test = np.linspace(0, 1, 100).reshape(-1, 1)\n\n# Predicciones de la PINN\ny_pred = model.predict(x_test)\n\n# Solución exacta\ny_exact = exact_solution(x_test)\n\n# Calcular el error\nerror = np.abs(y_pred - y_exact)\nmax_error = np.max(error)\nmean_error = np.mean(error)\n\nprint(\"Resultados:\")\nprint(f\"Error máximo: {max_error:.6f}\")\nprint(f\"Error medio: {mean_error:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T03:17:43.630623Z","iopub.execute_input":"2025-11-19T03:17:43.630956Z","iopub.status.idle":"2025-11-19T03:17:43.904906Z","shell.execute_reply.started":"2025-11-19T03:17:43.630923Z","shell.execute_reply":"2025-11-19T03:17:43.903727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluación de Resultados**\n\nGeneramos predicciones en 100 puntos equiespaciados y comparamos con la solución exacta:\n\n- **Error máximo**: Peor caso de aproximación\n- **Error medio**: Error promedio en todo el dominio\n\nEstas métricas nos dicen qué tan bien aprendió la PINN a resolver la ecuación diferencial.","metadata":{}},{"cell_type":"code","source":"# Gráficas para comparar resultados\nplt.figure(figsize=(15, 5))\n\n# Gráfica 1: Soluciones\nplt.subplot(1, 3, 1)\nplt.plot(x_test, y_exact, 'b-', label='Solución Exacta', linewidth=3)\nplt.plot(x_test, y_pred, 'r--', label='PINN', linewidth=2)\nplt.xlabel('x')\nplt.ylabel('y(x)')\nplt.title('Comparación: PINN vs Solución Exacta')\nplt.legend()\nplt.grid(True)\n\n# Gráfica 2: Error\nplt.subplot(1, 3, 2)\nplt.plot(x_test, error, 'g-', label='Error absoluto', linewidth=2)\nplt.xlabel('x')\nplt.ylabel('Error')\nplt.title('Error de Aproximación')\nplt.legend()\nplt.grid(True)\n\n# Gráfica 3: Evolución de la pérdida\nplt.subplot(1, 3, 3)\nplt.plot(losses, 'purple')\nplt.xlabel('Época')\nplt.ylabel('Pérdida')\nplt.title('Evolución de la Pérdida durante el Entrenamiento')\nplt.yscale('log')  # Escala logarítmica para ver mejor la convergencia\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T03:17:43.906135Z","iopub.execute_input":"2025-11-19T03:17:43.906411Z","iopub.status.idle":"2025-11-19T03:17:45.207377Z","shell.execute_reply.started":"2025-11-19T03:17:43.906390Z","shell.execute_reply":"2025-11-19T03:17:45.206223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Graficar puntos de entrenamiento\nplt.figure(figsize=(12, 4))\n\n# Puntos de entrenamiento\nx_train_vis = np.random.rand(1000, 1)\ny_train_vis = exact_solution(x_train_vis)\n\nplt.subplot(1, 2, 1)\nplt.scatter(x_train_vis, np.zeros_like(x_train_vis), alpha=0.6, color='red', s=10)\nplt.xlabel('x')\nplt.title('Ubicación de puntos de entrenamiento')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.scatter(x_train_vis, y_train_vis, alpha=0.4, color='red', s=15, label='Puntos entrenamiento')\nplt.plot(x_test, y_exact, 'b-', linewidth=2, label='Solución exacta')\nplt.xlabel('x')\nplt.ylabel('y(x)')\nplt.title('Puntos usados para entrenar PINN')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T03:17:45.209890Z","iopub.execute_input":"2025-11-19T03:17:45.210149Z","iopub.status.idle":"2025-11-19T03:17:45.656622Z","shell.execute_reply.started":"2025-11-19T03:17:45.210130Z","shell.execute_reply":"2025-11-19T03:17:45.655549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Resultados y Conclusiones:**\n\nLos resultados muestran que la PINN es capaz de aproximar con alta precisión la solución analítica del sistema, presentando una superposición casi exacta entre la predicción de la red y la solución cerrada. La evolución de la función de pérdida evidencia una convergencia estable durante el entrenamiento, mientras que el análisis del error absoluto confirma que la red mantiene discrepancias pequeñas y acotadas en todo el dominio evaluado.\n\nEstos hallazgos demuestran la eficacia de las PINNs para resolver ecuaciones diferenciales incluso sin datos experimentales, aprovechando directamente la estructura matemática del problema. La metodología utilizada constituye una base sólida para extender este enfoque hacia ecuaciones diferenciales más complejas, sistemas de EDOs o incluso ecuaciones en derivadas parciales, resaltando el potencial de las PINNs como herramienta computacional moderna en la modelación científica.","metadata":{}}]}